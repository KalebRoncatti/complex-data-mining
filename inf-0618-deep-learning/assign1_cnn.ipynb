{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Exploring CNNs on CIFAR-10 dataset\n",
    "### Equipe:\n",
    "- Kaleb Roncatti de Souza\n",
    "- Nelson Gomes Brasil Junior\n",
    "\n",
    "Todas as atividades são iniciadas em código por:\n",
    "```\n",
    "#################################################################\n",
    "## Atividade X. Activity description\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import models\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import visualkeras\n",
    "import os\n",
    "import random as rn\n",
    "from keras import callbacks\n",
    "from keras.activations import leaky_relu, relu, sigmoid\n",
    "\n",
    "\n",
    "rs = 321\n",
    "# Setting up random state to specific seed so we can have reproductibility\n",
    "os.environ['PYTHONHASHSEED']=str(rs)\n",
    "np.random.seed(rs)\n",
    "tf.random.set_seed(rs)\n",
    "rn.seed(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train and test set\n",
    "(x_train , y_train), (x_test , y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Device that will be used to train the deep learning models\n",
    "device = '/gpu:0' # or '/cpu:0' if you don't have GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shuffle\n",
    "def shuffle_tensor(x: np.ndarray , y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    indices = tf.range(start=0, limit=tf.shape(x)[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices, seed=rs)\n",
    "    return tf.gather(x, shuffled_indices), tf.gather(y, shuffled_indices)\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(x: np.ndarray, y: np.ndarray, percentage: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"array x and y must have the same length\")\n",
    "    \n",
    "    # Shuffling dataset\n",
    "    x, y = shuffle_tensor(x=x, y=y)\n",
    "\n",
    "    # Finding the splits after shuffling\n",
    "    total_elements = len(y)\n",
    "    data = int(percentage * total_elements)\n",
    "    return x[0:data], y[0:data], x[data:], y[data:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot accuracy and loss (train/validation)\n",
    "def plot_train_val_acc_loss(model: tf.keras.Sequential(), model_name: str):\n",
    "    plt.plot(model.history.history['accuracy'])\n",
    "    plt.plot(model.history.history['val_accuracy'])\n",
    "    plt.title(f'model accuracy ({model_name})')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(model.history.history['loss'])\n",
    "    plt.plot(model.history.history['val_loss'])\n",
    "    plt.title(f'model loss ({model_name})')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 1. Splitting training and validation\n",
    "x_train, y_train, x_val, y_val = split_dataset(x=x_train, y=y_train, percentage = 0.8)\n",
    "print(f\"We splitted the training dataset using the following percentage: {len(x_train)/(len(x_train) + len(x_val))}\")\n",
    "print(f\"Training dataset has the following shape: x ==> {x_train.shape}, y ==> {y_train.shape}\")\n",
    "print(f\"Validation dataset has the following shape: x ==> {x_val.shape}, y ==> {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data between [0, 1]\n",
    "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
    "x_val = tf.cast(x_val, tf.float32) / 255.0\n",
    "x_test = tf.cast(x_test, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "# One hot encoding on labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_oh = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_oh = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape, y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if we have a balanced dataset\n",
    "# Para o conjunto de treino\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train ---> \", dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(\"Validation ---> \", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider that the dataset is more or less balanced. We could also adjust the weights based on their proportion, but for simplicity, let's keep going considering the dataset is completely balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 2. Creating a very simple convolutional neural net\n",
    "\n",
    "def create_baseline_model(input_shape=x_train[0].shape, activation = relu) -> tf.keras.Sequential():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Convolutional layer with 32 filters, 3x3 each\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=activation, \n",
    "            input_shape=x_train[0].shape))\n",
    "    #Max pooling of size 3x3\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_baseline_model()\n",
    "model.summary()\n",
    "# Taking a look at the neural net\n",
    "visualkeras.layered_view(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the baseline model and saving it\n",
    "with tf.device(device):\n",
    "    model.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_baseline.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the model accuracy through the epochs\n",
    "plot_train_val_acc_loss(model=model, model_name=f\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 3. Exploring OTHER two activation types, since we already played with ReLU\n",
    "\n",
    "model_sigmoid = create_baseline_model(activation=sigmoid)\n",
    "model_sigmoid.summary()\n",
    "# Compiling the model\n",
    "model_sigmoid.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_leaky_relu = create_baseline_model(activation=leaky_relu)\n",
    "model_leaky_relu.summary()\n",
    "# Compiling the model\n",
    "model_leaky_relu.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the baseline model and saving it with OTHER ACTIVATIONS\n",
    "# Sigmoid\n",
    "with tf.device(device):\n",
    "    model_sigmoid.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_baseline_sigmoid.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )\n",
    "        \n",
    "# Leaky ReLU\n",
    "with tf.device(device):\n",
    "    model_leaky_relu.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_baseline_leaky_relu.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the model accuracy through the epochs for SIGMOID\n",
    "plot_train_val_acc_loss(model=model_sigmoid, model_name=f\"Baseline Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the model accuracy through the epochs for LeakyReLU\n",
    "plot_train_val_acc_loss(model=model_leaky_relu, model_name=f\"Baseline LeakyReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 4. Exploring more complex CNNs to improve our model\n",
    "\n",
    "model_complex = tf.keras.Sequential()\n",
    "\n",
    "model_complex.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu, \n",
    "        input_shape=x_train[0].shape))\n",
    "model_complex.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu))\n",
    "\n",
    "model_complex.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flattening\n",
    "model_complex.add(layers.Flatten())\n",
    "# Output layer\n",
    "model_complex.add(layers.Dense(10))\n",
    "\n",
    "model_complex.summary()\n",
    "# Compiling the model\n",
    "model_complex.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Taking a look at the neural net\n",
    "visualkeras.layered_view(model_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    model_complex.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_complex.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the model accuracy through the epochs for model complex 0\n",
    "plot_train_val_acc_loss(model=model_complex, model_name=f\"Model Complex 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complex_1 = tf.keras.Sequential()\n",
    "\n",
    "model_complex_1.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu, \n",
    "        input_shape=x_train[0].shape))\n",
    "model_complex_1.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu))\n",
    "\n",
    "model_complex_1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_complex_1.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu, \n",
    "        input_shape=x_train[0].shape))\n",
    "model_complex_1.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu))\n",
    "\n",
    "model_complex_1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening\n",
    "model_complex_1.add(layers.Flatten())\n",
    "# Output layer\n",
    "model_complex_1.add(layers.Dense(10))\n",
    "\n",
    "model_complex_1.summary()\n",
    "# Compiling the model\n",
    "model_complex_1.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Taking a look at the neural net\n",
    "visualkeras.layered_view(model_complex_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    model_complex_1.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_complex_1.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observing the model accuracy through the epochs for model complex 1\n",
    "plot_train_val_acc_loss(model=model_complex_1, model_name=f\"Model Complex 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 5. Exploring different types of initialization and regularization\n",
    "\n",
    "## Varying the initialization ##\n",
    "# We are going to use the same models as model_complex_1\n",
    "# Since the glorot_uniform is the default, we are going to test\n",
    "# [\"random_normal\", \"ones\"]\n",
    "\n",
    "\n",
    "inits = [\"random_normal\", \"he_normal\"]\n",
    "models_init = []\n",
    "for initialization in inits:\n",
    "\n",
    "    model_tmp = tf.keras.Sequential()\n",
    "\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu, \n",
    "            input_shape=x_train[0].shape,\n",
    "            kernel_initializer=initialization))\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu,\n",
    "            kernel_initializer=initialization))\n",
    "\n",
    "    model_tmp.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu, \n",
    "            input_shape=x_train[0].shape,\n",
    "            kernel_initializer=initialization))\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu,\n",
    "            kernel_initializer=initialization))\n",
    "\n",
    "    model_tmp.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flattening\n",
    "    model_tmp.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model_tmp.add(layers.Dense(10, kernel_initializer=initialization))\n",
    "\n",
    "    model_tmp.summary()\n",
    "    # Compiling the model\n",
    "    model_tmp.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Taking a look at the neural net\n",
    "    visualkeras.layered_view(model_tmp)\n",
    "\n",
    "    # Training model\n",
    "    with tf.device(device):\n",
    "        model_tmp.fit(\n",
    "            x_train, y_train_oh, \n",
    "            epochs=30, \n",
    "            batch_size=64,\n",
    "            validation_data=(x_val, y_val_oh),\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                callbacks.TerminateOnNaN(),\n",
    "                callbacks.ModelCheckpoint(\n",
    "                    f'weights_complex_1_init-{initialization}.h5',\n",
    "                    save_best_only=True,\n",
    "                    verbose=1),\n",
    "            ]\n",
    "            )\n",
    "\n",
    "    models_init.append(model_tmp)\n",
    "    # Observing results\n",
    "    plot_train_val_acc_loss(model=model_tmp, model_name=f\"Model Complex 1 Init - {initialization}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Varying the regularization using the default initializer (Glorot/Bengio: glorot_uniform) ##\n",
    "# We are going to use the same models as model_complex_1\n",
    "\n",
    "\n",
    "regs = [regularizers.l2(0.001), regularizers.l2(0.01)]\n",
    "regs_str = [\"l2-0.001\", \"l2-0.01\"]\n",
    "models_reg = []\n",
    "acc = 0\n",
    "for regularization in regs:\n",
    "\n",
    "    model_tmp = tf.keras.Sequential()\n",
    "\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu, \n",
    "            input_shape=x_train[0].shape,\n",
    "            kernel_regularizer=regularization))\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=32, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu,\n",
    "            kernel_regularizer=regularization))\n",
    "\n",
    "    model_tmp.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu, \n",
    "            input_shape=x_train[0].shape,\n",
    "            kernel_regularizer=regularization))\n",
    "    model_tmp.add(\n",
    "        layers.Conv2D(\n",
    "            filters=16, \n",
    "            kernel_size=(3, 3), \n",
    "            padding='valid', \n",
    "            activation=leaky_relu,\n",
    "            kernel_regularizer=regularization))\n",
    "\n",
    "    model_tmp.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flattening\n",
    "    model_tmp.add(layers.Flatten())\n",
    "    # Output layer\n",
    "    model_tmp.add(layers.Dense(10, kernel_regularizer=regularization))\n",
    "\n",
    "    model_tmp.summary()\n",
    "    # Compiling the model\n",
    "    model_tmp.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Taking a look at the neural net\n",
    "    visualkeras.layered_view(model_tmp)\n",
    "\n",
    "    # Training model\n",
    "    with tf.device(device):\n",
    "        model_tmp.fit(\n",
    "            x_train, y_train_oh, \n",
    "            epochs=30, \n",
    "            batch_size=64,\n",
    "            validation_data=(x_val, y_val_oh),\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                callbacks.TerminateOnNaN(),\n",
    "                callbacks.ModelCheckpoint(\n",
    "                    f'weights_complex_1_reg-{regs_str[acc]}.h5',\n",
    "                    save_best_only=True,\n",
    "                    verbose=1),\n",
    "            ]\n",
    "            )\n",
    "    \n",
    "    models_reg.append(model_tmp)\n",
    "    \n",
    "    # Observing results\n",
    "    plot_train_val_acc_loss(model=model_tmp, model_name=f\"Model Complex 1 Reg - {regs_str[acc]}\")\n",
    "    acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 6. Exploring Droupout\n",
    "# Using it before the fully connected (dense) layer as suggested by Hinton (2012)\n",
    "# https://arxiv.org/pdf/1207.0580.pdf\n",
    "\n",
    "model_complex_1_dropout = tf.keras.Sequential()\n",
    "\n",
    "model_complex_1_dropout.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu, \n",
    "        input_shape=x_train[0].shape))\n",
    "model_complex_1_dropout.add(\n",
    "    layers.Conv2D(\n",
    "        filters=32, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu))\n",
    "\n",
    "model_complex_1_dropout.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model_complex_1_dropout.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu, \n",
    "        input_shape=x_train[0].shape))\n",
    "model_complex_1_dropout.add(\n",
    "    layers.Conv2D(\n",
    "        filters=16, \n",
    "        kernel_size=(3, 3), \n",
    "        padding='valid', \n",
    "        activation=leaky_relu))\n",
    "\n",
    "model_complex_1_dropout.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening\n",
    "model_complex_1_dropout.add(layers.Flatten())\n",
    "\n",
    "# Dropout layer\n",
    "model_complex_1_dropout.add(layers.Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model_complex_1_dropout.add(layers.Dense(10))\n",
    "\n",
    "model_complex_1_dropout.summary()\n",
    "# Compiling the model\n",
    "model_complex_1_dropout.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Taking a look at the neural net\n",
    "visualkeras.layered_view(model_complex_1_dropout)\n",
    "\n",
    "with tf.device(device):\n",
    "    model_complex_1_dropout.fit(\n",
    "        x_train, y_train_oh, \n",
    "        epochs=30, \n",
    "        batch_size=64,\n",
    "        validation_data=(x_val, y_val_oh),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            callbacks.TerminateOnNaN(),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                'weights_complex_1_dropout.h5',\n",
    "                save_best_only=True,\n",
    "                verbose=1),\n",
    "        ]\n",
    "        )\n",
    "\n",
    "# Observing the model accuracy\n",
    "plot_train_val_acc_loss(model=model_complex_1_dropout, model_name=\"Model Complex 1 Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "## Atividade 7. Plotting Loss VS Epochs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bffa78d1b7559c64ab888610ccd57317b93a460d1e9e875ec30014adf19d377b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
